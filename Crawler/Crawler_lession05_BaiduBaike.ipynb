{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: scrape Baidu Baike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "base_url = \"https://baike.baidu.com\"\n",
    "his = [\"/item/%E9%99%88%E5%A5%95%E8%BF%85/128029\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>陈奕迅</h1> url: /item/%E9%99%88%E5%A5%95%E8%BF%85/128029\n"
     ]
    }
   ],
   "source": [
    "url = base_url + his[-1]\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, features = 'lxml' )\n",
    "print(soup.find('h1'), 'url:', his[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sub_urls for baidu baike (item page), randomly select a sub_urls and store it in \"his\". If no valid sub link is found, than pop last url in \"his\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/item/%E9%99%88%E5%A5%95%E8%BF%85/128029', '/item/%E5%85%84%E5%BC%9F']\n"
     ]
    }
   ],
   "source": [
    "# find valid urls\n",
    "sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "\n",
    "if len(sub_urls) != 0:\n",
    "    his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "else:\n",
    "    # no valid sub link found\n",
    "    his.pop()\n",
    "print(his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 陈奕迅     url:  /item/%E9%99%88%E5%A5%95%E8%BF%85/128029\n",
      "1 张婉婷     url:  /item/%E5%BC%A0%E5%A9%89%E5%A9%B7\n",
      "2 宋家皇朝     url:  /item/%E5%AE%8B%E5%AE%B6%E7%9A%87%E6%9C%9D\n",
      "3 嘉禾电影     url:  /item/%E5%98%89%E7%A6%BE%E7%94%B5%E5%BD%B1%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8\n",
      "4 鹿鼎记     url:  /item/%E9%B9%BF%E9%BC%8E%E8%AE%B0\n",
      "5 赵述仁     url:  /item/%E8%B5%B5%E8%BF%B0%E4%BB%81\n",
      "6 张鸿斌     url:  /item/%E5%BC%A0%E9%B8%BF%E6%96%8C\n",
      "7 百度百科：多义词     url:  /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%A4%9A%E4%B9%89%E8%AF%8D\n",
      "8 赵氏孤儿     url:  /item/%E8%B5%B5%E6%B0%8F%E5%AD%A4%E5%84%BF\n",
      "9 程婴     url:  /item/%E7%A8%8B%E5%A9%B4\n",
      "10 赵宣     url:  /item/%E8%B5%B5%E5%AE%A3\n",
      "11 百度百科：多义词     url:  /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%A4%9A%E4%B9%89%E8%AF%8D\n",
      "12 百度百科     url:  /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91\n",
      "13 百科任务评审团     url:  /item/%E7%99%BE%E7%A7%91%E4%BB%BB%E5%8A%A1%E8%AF%84%E5%AE%A1%E5%9B%A2\n",
      "14 百科评审团评审规则     url:  /item/%E7%99%BE%E7%A7%91%E8%AF%84%E5%AE%A1%E5%9B%A2%E8%AF%84%E5%AE%A1%E8%A7%84%E5%88%99\n",
      "15 百度百科：词条概述     url:  /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E8%AF%8D%E6%9D%A1%E6%A6%82%E8%BF%B0\n",
      "16 百科评审团评审规则     url:  /item/%E7%99%BE%E7%A7%91%E8%AF%84%E5%AE%A1%E5%9B%A2%E8%AF%84%E5%AE%A1%E8%A7%84%E5%88%99\n",
      "17 百度百科：参考资料     url:  /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99\n",
      "18 百度百科     url:  /item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91\n",
      "19 百科核心用户     url:  /item/%E7%99%BE%E7%A7%91%E6%A0%B8%E5%BF%83%E7%94%A8%E6%88%B7\n"
     ]
    }
   ],
   "source": [
    "# Put everthing together. Random running for 20 iterations. See what we end up with.\n",
    "his = [\"/item/%E9%99%88%E5%A5%95%E8%BF%85/128029\"]\n",
    "\n",
    "for i in range(20):\n",
    "    url = base_url + his[-1]\n",
    "\n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    print(i, soup.find('h1').get_text(), '    url: ', his[-1])\n",
    "\n",
    "    # find valid urls\n",
    "    sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "\n",
    "    if len(sub_urls) != 0:\n",
    "        his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "    else:\n",
    "        # no valid sub link found\n",
    "        his.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
